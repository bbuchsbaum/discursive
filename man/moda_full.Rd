% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/moda.R
\name{moda_full}
\alias{moda_full}
\title{Multimodal Oriented Discriminant Analysis (MODA) - Complete & Faithful Implementation}
\usage{
moda_full(
  X,
  y,
  k,
  numClusters = 1,
  pcaFirst = TRUE,
  pcaVar = 0.95,
  maxIter = 50,
  tol = 1e-05,
  clusterMethod = "kmeans",
  B_init = "random",
  verbose = FALSE,
  lineSearchIter = 20,
  B_init_sd = 0.01
)
}
\arguments{
\item{X}{A numeric matrix of size \eqn{d \times n}, where each column is a data sample.}

\item{y}{A vector (length \eqn{n}) of integer or factor class labels (must have \(\geq 2\) distinct labels).}

\item{k}{Integer. Dimensionality of the target subspace (number of features to extract).}

\item{numClusters}{Integer or vector/list specifying #clusters per class. If 1, it's ODA.}

\item{pcaFirst}{Logical. If TRUE, run PCA first to reduce dimension if \eqn{d >> n}. Defaults to TRUE.}

\item{pcaVar}{Fraction of variance to keep if \code{pcaFirst=TRUE}. Defaults to 0.95.}

\item{maxIter}{Maximum number of majorization iterations. Defaults to 50.}

\item{tol}{Convergence tolerance on relative change in the objective \eqn{G(\mathbf{B})}. Defaults to 1e-5.}

\item{clusterMethod}{Either \code{"kmeans"} or a custom function accepting (dataMatrix, kC) -> clusterIDs.}

\item{B_init}{Either \code{"random"} or \code{"pca"} to initialize the projection matrix \(\mathbf{B}\).}

\item{verbose}{If TRUE, prints iteration progress.}

\item{lineSearchIter}{Number of line search iterations for step size selection (default = 20).}

\item{B_init_sd}{Standard deviation for the random initialization of \(\mathbf{B}\) if \code{B_init="random"}. Defaults to 1e-2.}
}
\value{
A list with elements:
\itemize{
\item \code{B}: A \eqn{d' \times k} matrix (or \eqn{d \times k} if no PCA) with the learned projection.
\item \code{objVals}: The values of the objective \eqn{G(\mathbf{B})} at each iteration.
\item \code{clusters}: The cluster assignments (per class).
\item \code{pcaInfo}: If PCA was applied, contains the PCA rotation \code{U} and mean.
}
}
\description{
Implements the full Multimodal Oriented Discriminant Analysis (MODA) framework as
derived in De la Torre & Kanade (2005). This code:
\enumerate{
\item \strong{Clusters each class} into one or more clusters to capture multimodal structure.
\item \strong{Approximates each cluster's covariance} as \eqn{U_i \Lambda_i U_i^T + \sigma_i^2 I}
to handle high-dimensional data (Section 6 of the paper).
\item \strong{Constructs the majorization function} \eqn{L(\mathbf{B})} that upper-bounds
the Kullback–Leibler divergence-based objective \eqn{G(\mathbf{B})} (Equations (7)-(8)).
\item \strong{Iterates} using the gradient-based solution to minimize \eqn{E_5(\mathbf{B})}
(Equation (10)) with updates from Equation (11) (i.e., normalized gradient descent
or line search).
}

It \strong{does not} merely provide a starter approach; instead, it faithfully implements
the steps described in the paper, including references to Equations (7)–(11).
}
\details{
\strong{Key Steps}:
\enumerate{
\item \strong{Clustering} (Section 4): For each class, optionally split the samples into
multiple clusters to model multimodality.
\item \strong{Approximate Covariances} (Section 6): For each cluster, approximate
\(\Sigma_i\) by \(\mathbf{U}_i \boldsymbol{\Lambda}_i \mathbf{U}_i^T + \sigma_i^2 \mathbf{I}\).
\item \strong{Majorization} (Sections 5.1–5.2): Build \eqn{L(\mathbf{B})} from \eqn{G(\mathbf{B})}
using Equation (7) and sum up to get Equation (8).
\item \strong{Iterative Minimization} of \eqn{L(\mathbf{B})} \(\geq G(\mathbf{B})\). The partial
derivatives (Equation (9)) yield a system of linear equations, solved here by
gradient-based updates (Equations (10)–(11)).
}

\strong{High-Dimensional Data}: When \eqn{d \gg n}, it is recommended to set \code{pcaFirst=TRUE}
so that the dimension is reduced to at most \eqn{n}, avoiding rank deficiency and
improving generalization.

\strong{Classification after MODA}: Once \eqn{B} is learned, map a new sample \eqn{\mathbf{x}}
to \eqn{\mathbf{B}^T \mathbf{x}} (plus PCA if used) and classify in that lower-dimensional space.

For further details, see:
\enumerate{
\item De la Torre & Kanade (2005). "Multimodal Oriented Discriminant Analysis."
\item Equations (7)–(11) for the majorization steps.
\item Section 6 for the covariance factorization in high dimensions.
}
}
\section{References to the Paper}{

\itemize{
\item \strong{Equation (7)}: Inequality used to construct the majorization function.
\item \strong{Equation (8)}: Definition of \eqn{L(\mathbf{B})} that majorizes \eqn{G(\mathbf{B})}.
\item \strong{Equation (9)}: Necessary condition for the minimum of \eqn{L(\mathbf{B})}.
\item \strong{Equation (10)}: Definition of \eqn{E_5(\mathbf{B})} to be minimized via gradient methods.
\item \strong{Equation (11)}: Normalized gradient-descent update with step size \eqn{\eta} chosen
to minimize \eqn{E_5}.
}
}

\examples{
# Synthetic example (small scale):
set.seed(123)
d <- 20; n <- 40
X <- matrix(rnorm(d*n), nrow = d, ncol = n)
y <- rep(1:2, each = n/2)
res <- moda_full(X, y, k = 2, numClusters = 1, pcaFirst = FALSE, maxIter = 15, verbose = TRUE)

# Inspect the learned projection B
str(res)

}
