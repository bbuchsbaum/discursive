% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/em_soft_lda.R, R/soft_lda.R
\name{soft_lda}
\alias{soft_lda}
\title{Partially Supervised LDA with Soft Labels (Robust Version)}
\usage{
soft_lda(
  X,
  C,
  preproc = pass(),
  dp = min(dim(X)),
  di = dp - 1,
  dl = ncol(C) - 1
)

soft_lda(
  X,
  C,
  preproc = pass(),
  dp = min(dim(X)),
  di = dp - 1,
  dl = ncol(C) - 1
)
}
\arguments{
\item{X}{A numeric matrix of size \code{n x d}, where \code{n} is the number of samples and \code{d} is the number of features.}

\item{C}{A numeric matrix of size \code{n x c}, where \code{c} is the number of classes. Each element \code{C[i, j]} represents
the membership weight of sample \code{i} to class \code{j}. Weights must be non-negative, and \code{nrow(C) = nrow(X)}.}

\item{preproc}{A preprocessing function from \code{multivarious} such as \code{center()} or \code{pass()}. Defaults to \code{pass()},
which does no preprocessing.}

\item{dp}{Integer. The number of principal components to retain in the initial PCA reduction. Default is \code{min(dim(X))}.}

\item{di}{Integer. The dimension of the within-class projection. Default is \code{dp - 1}.}

\item{dl}{Integer. The dimension of the between-class projection. Default is \code{ncol(C) - 1}, which is one less than the number of classes.}

\item{PL}{A numeric matrix n x K of plausibility values for each class and instance.}

\item{max_iter}{Integer, maximum number of E²M iterations. Default: 100.}

\item{tol}{Numeric tolerance for convergence in the log-likelihood. Default: 1e-6.}

\item{n_starts}{Integer, number of random initializations. The best solution (highest final log-likelihood) is chosen. Default: 5.}

\item{reg}{Numeric, a small ridge penalty to add to the covariance matrix for numerical stability. Default: 1e-9.}

\item{verbose}{Logical, if TRUE prints progress messages. Default: FALSE.}
}
\value{
A list with:
\item{pi}{Estimated class priors}
\item{mu}{Estimated class means}
\item{Sigma}{Estimated covariance matrix}
\item{zeta}{Posterior class probabilities (n x K)}
\item{loglik}{Final log evidential likelihood}
\item{iter}{Number of iterations performed}

An object of class \code{projector}, compatible with \code{multivarious}, containing:
\itemize{
\item \code{preproc}: The preprocessing object used.
\item \code{ncomp}: The number of components in the final projection.
\item \code{v}: The final projection matrix of size \code{d x dl}.
\item \code{classes}: "sl_lda" indicating the type of projector.
}
}
\description{
Fits a Linear Discriminant Analysis (LDA) model using soft labels
and the Evidential EM (E²M) algorithm, with improved robustness and optional regularization.

This function implements a soft-label variant of Linear Discriminant Analysis (LDA), as described in related literature
on semi-supervised dimensionality reduction. Instead of assuming hard class assignments for each sample, it allows for
each sample to have fractional (soft) memberships across multiple classes. This approach can be beneficial when class
labels are uncertain, noisy, or multi-labeled.
}
\details{
The procedure integrates a PCA-based dimensionality reduction step (to \code{dp} components) followed by a two-step LDA-like
projection. The final result is a linear projector that maps the data into a lower-dimensional space maximizing between-class
variation and minimizing within-class variation under the soft-label regime.

The \code{soft_lda} function extends traditional LDA to the case where class assignments are soft. It follows these steps:
\enumerate{
\item \strong{Preprocessing}: Apply \code{preproc} to the data \code{X}.
\item \strong{PCA Projection}: Reduce the dimensionality of \code{X} to \code{dp} components.
\item \strong{Construct Weight Matrices}: The matrix \code{C} defines soft memberships. From \code{C}, we derive:
\itemize{
\item \code{E}: A diagonal matrix with row sums of \code{C}.
\item \code{G}: A diagonal matrix with column sums of \code{C}, representing class weights.
\item \code{F}: The transpose of \code{C}, so each row corresponds to a class.
}
\item \strong{Scatter Matrices}: Using the weighted assignments, within-class scatter (\code{Sw}) and between-class scatter (\code{Sb}) are computed.
\item \strong{Intermediate Projection (di)}: Perform an eigen-decomposition on \code{Sw} to get a \code{di}-dimensional subspace that captures within-class structure.
\item \strong{Final Projection (dl)}: Project class means into the \code{di}-dim space, then run PCA to extract the top \code{dl} directions that best discriminate classes.
\item \strong{Combine Projections}: Multiply the PCA projection \code{proj_dp}, the \code{di}-dimension projection \code{proj_di}, and the \code{dl}-dimension projection \code{proj_dl} to get the final projection.
}

This approach is influenced by methods described in:
\itemize{
\item \emph{A general soft label based Linear Discriminant Analysis for semi-supervised dimensionality reduction}
\item \emph{A weighted linear discriminant analysis framework for multi-label feature extraction}
}
}
\examples{
set.seed(123)
n <- 100; d <- 2; K <- 3
X <- rbind(
  MASS::mvrnorm(n, c(1,0), diag(2)),
  MASS::mvrnorm(n, c(-1,1), diag(2)),
  MASS::mvrnorm(n, c(0,-1), diag(2))
)
Y <- c(rep(1,n), rep(2,n), rep(3,n))
# Soft labels: add uncertainty
PL <- matrix(0, 3*n, K)
for (i in 1:(3*n)) {
  if (runif(1)<0.2) {
    alt <- sample(setdiff(1:K, Y[i]),1)
    PL[i,Y[i]] <- 0.5
    PL[i,alt] <- 0.5
  } else {
    PL[i,Y[i]] <- 1
  }
}
res <- soft_lda(X, PL, verbose=TRUE)
str(res)
}
\references{
Quost, B., Denoeux, T., Li, S. (2017). Parametric classification with soft labels
using the Evidential EM algorithm. Advances in Data Analysis and Classification,
11(4), 659-690.

For example reference, see:
\url{https://paperpile.com/app/p/03eb8cb3-2326-0cc5-a440-02c6aa543bf3}
}
