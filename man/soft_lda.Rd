% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/em_soft_lda.R, R/soft_lda.R
\name{soft_lda}
\alias{soft_lda}
\title{Partially Supervised LDA with Soft Labels (Robust Version)}
\usage{
soft_lda(
  X,
  C,
  preproc = pass(),
  dp = min(dim(X)),
  di = dp - 1,
  dl = ncol(C) - 1,
  alpha = 0
)

soft_lda(
  X,
  C,
  preproc = pass(),
  dp = min(dim(X)),
  di = dp - 1,
  dl = ncol(C) - 1,
  alpha = 0
)
}
\arguments{
\item{X}{A numeric matrix \(\mathrm{n \times d}\), rows = samples, columns = features.}

\item{C}{A numeric matrix \(\mathrm{n \times c}\) of soft memberships.
\code{C[i, j]} = weight of sample \code{i} for class \code{j}.
Must be \(\ge 0\); row sums can be any positive value
(if \(\sum_j C\link{i,j} = 1\), each row is a probability distribution).}

\item{preproc}{A \code{pre_processor} from \pkg{multivarious}, e.g. \code{center()} or \code{pass()}.
Defaults to \code{pass()} (no centering).}

\item{dp}{Integer. Number of principal components to keep in the first PCA step.
Defaults to \code{min(dim(X))}.}

\item{di}{Integer. Dimension of the \emph{within-class} subspace. Default \code{dp - 1}.}

\item{dl}{Integer. Dimension of the final subspace for \emph{between-class} separation.
Default \code{ncol(C) - 1}.}

\item{alpha}{A numeric ridge parameter (\(\ge 0\)). If \code{alpha > 0}, we add \(\alpha I\)
to \(\widetilde{S}_w\) to ensure invertibility. Default \code{0}.}

\item{PL}{A numeric matrix n x K of plausibility values for each class and instance.}

\item{max_iter}{Integer, maximum number of E²M iterations. Default: 100.}

\item{tol}{Numeric tolerance for convergence in the log-likelihood. Default: 1e-6.}

\item{n_starts}{Integer, number of random initializations. The best solution (highest final log-likelihood) is chosen. Default: 5.}

\item{reg}{Numeric, a small ridge penalty to add to the covariance matrix for numerical stability. Default: 1e-9.}

\item{verbose}{Logical, if TRUE prints progress messages. Default: FALSE.}
}
\value{
A list with:
\item{pi}{Estimated class priors}
\item{mu}{Estimated class means}
\item{Sigma}{Estimated covariance matrix}
\item{zeta}{Posterior class probabilities (n x K)}
\item{loglik}{Final log evidential likelihood}
\item{iter}{Number of iterations performed}

A \code{\link[multivarious]{discriminant_projector}} object with subclass
\code{"soft_lda"} containing:
\itemize{
\item \code{v} ~ The \(\mathrm{(d \times dl)}\) final projection matrix.
\item \code{s} ~ The \(\mathrm{(n \times dl)}\) projected scores of the training set.
\item \code{sdev} ~ The std dev of each dimension in \code{s}.
\item \code{labels} ~ Currently set to \code{colnames(C)} (or \code{NULL}).
\item \code{preproc} ~ The preprocessing object used.
\item \code{classes} ~ A string \code{"soft_lda"}.
}
}
\description{
Fits a Linear Discriminant Analysis (LDA) model using soft labels
and the Evidential EM (E²M) algorithm, with improved robustness and optional regularization.

This function implements a \emph{soft-label} variant of Linear Discriminant Analysis (LDA),
following the approach described in:
}
\details{
Zhao, M., Zhang, Z., Chow, T.W.S., & Li, B. (2014).
"A general soft label based Linear Discriminant Analysis for semi-supervised dimensionality reduction."
\emph{Neurocomputing, 135}, 250-264.

Instead of hard (0/1) labels, each sample can have fractional memberships (soft labels) across
multiple classes. These memberships are encoded in a matrix \code{C}, typically obtained via
a label-propagation or fuzzy labeling step. SL-LDA uses these soft memberships to form
generalized scatter matrices \(\widetilde{S}_w\) and \(\widetilde{S}_b\), then solves
an LDA-like dimension-reduction problem in a PCA subspace via a \emph{two-step} approach:

\enumerate{
\item \strong{Preprocessing}:
Apply a \code{preproc} function (e.g. \code{center()}) to the data \code{X}.
\item \strong{PCA}:
Project the data onto the top \code{dp} principal components (to handle rank deficiency).
\item \strong{Compute Soft-Label Scatter} in the PCA space:
\itemize{
\item Let \(\mathbf{F} = \mathbf{C}^\top\) be size \(\mathrm{c \times n}\).
\item Let \(\mathbf{E} = \mathrm{diag}(\text{rowSums}(\mathbf{C}))\) (size \(\mathrm{n \times n}\)).
\item Let \(\mathbf{G} = \mathrm{diag}(\text{colSums}(\mathbf{C}))\) (size \(\mathrm{c \times c}\)).
\item Form \(\widetilde{S}_w = X_p^\top ( E - F^\top G^{-1} F ) X_p + \alpha I\) (within-class),
and \(\widetilde{S}_b = X_p^\top \bigl(F^\top G^{-1}F - \tfrac{E e e^\top E}{e\,E\,e^\top}\bigr) X_p\) (between-class).
}
\item \strong{Within-class projection} (\code{di}):
Partially diagonalize \(\widetilde{S}_w\).
In code, we extract \code{di} eigenvectors.
(\emph{Note}: Some references keep the \emph{largest} eigenvalues, others the \emph{smallest}.)
\item \strong{Between-class projection} (\code{dl}):
Project the (soft) class means into the \code{di}-dim subspace, then run a small PCA
for dimension \code{dl}.
\item \strong{Combine}:
Multiply \(\mathrm{(d \times dp)} \cdot (\mathrm{dp \times di}) \cdot (\mathrm{di \times dl})\)
to get the final \(\mathrm{(d \times dl)}\) projection matrix.
}

In typical references, one might pick the \emph{largest} eigenvalues of \(\widetilde{S}_w\)
for stable inversion, but certain versions (like Null-LDA) use the \emph{smallest} eigenvalues.
Adjust the code in \code{RSpectra::eigs_sym()} accordingly if you prefer a different variant.

If you want to confirm \(\widetilde{S}_t = \widetilde{S}_w + \widetilde{S}_b\) numerically,
you can define a helper function for \(\widetilde{S}_t\) and compare it to \(\widetilde{S}_w + \widetilde{S}_b\).
}
\examples{
set.seed(123)
n <- 100; d <- 2; K <- 3
X <- rbind(
  MASS::mvrnorm(n, c(1,0), diag(2)),
  MASS::mvrnorm(n, c(-1,1), diag(2)),
  MASS::mvrnorm(n, c(0,-1), diag(2))
)
Y <- c(rep(1,n), rep(2,n), rep(3,n))
# Soft labels: add uncertainty
PL <- matrix(0, 3*n, K)
for (i in 1:(3*n)) {
  if (runif(1)<0.2) {
    alt <- sample(setdiff(1:K, Y[i]),1)
    PL[i,Y[i]] <- 0.5
    PL[i,alt] <- 0.5
  } else {
    PL[i,Y[i]] <- 1
  }
}
res <- soft_lda(X, PL, verbose=TRUE)
str(res)
}
\references{
Quost, B., Denoeux, T., Li, S. (2017). Parametric classification with soft labels
using the Evidential EM algorithm. Advances in Data Analysis and Classification,
11(4), 659-690.

Zhao, M., Zhang, Z., Chow, T.W.S., & Li, B. (2014).
"A general soft label based Linear Discriminant Analysis for semi-supervised dimensionality reduction."
\emph{Neurocomputing, 135}, 250-264.
}
